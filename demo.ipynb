{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T18:16:20.654566Z",
     "start_time": "2025-03-31T18:16:20.239214Z"
    }
   },
   "source": [
    "!uv pip install yamlmagic\n",
    "%load_ext yamlmagic"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mAudited \u001B[1m1 package\u001B[0m \u001B[2min 3ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T18:15:48.104561Z",
     "start_time": "2025-03-31T18:15:47.878856Z"
    }
   },
   "source": [
    "from kubernetes import client, config, utils\n",
    "\n",
    "configuration = client.Configuration()\n",
    "config.load_kube_config(client_configuration=configuration)\n",
    "k8s_client = client.ApiClient(configuration)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T18:17:01.640632Z",
     "start_time": "2025-03-31T18:17:01.636729Z"
    }
   },
   "source": [
    "%%yaml output_pvc\n",
    "\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  labels:\n",
    "    opendatahub.io/dashboard: \"true\"\n",
    "  name: output\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 100Gi\n",
    "  storageClassName: nfs-csi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            require(\n                [\n                    \"notebook/js/codecell\",\n                    \"codemirror/mode/yaml/yaml\"\n                ],\n                function(cc){\n                    cc.CodeCell.options_default.highlight_modes.magic_yaml = {\n                        reg: [\"^%%yaml\"]\n                    }\n                }\n            );\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "utils.create_from_dict(k8s_client, namespace=\"llama-stack\", apply=True, data=output_pvc)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:00:17.286733Z",
     "start_time": "2025-03-28T09:00:17.283429Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from llama_stack_client.types.post_training_supervised_fine_tune_params import (\n",
    "    TrainingConfig,\n",
    "    TrainingConfigDataConfig,\n",
    "    TrainingConfigEfficiencyConfig,\n",
    "    TrainingConfigOptimizerConfig,\n",
    ")\n",
    "from llama_stack_client.types.algorithm_config_param import LoraFinetuningConfig\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:00:20.475583Z",
     "start_time": "2025-03-28T09:00:20.472902Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "algorithm_config = LoraFinetuningConfig(\n",
    "    type=\"LoRA\",\n",
    "    # List of which linear layers LoRA should be applied to in each self-attention block\n",
    "    # Options are {\"q_proj\", \"k_proj\", \"v_proj\", \"output_proj\"}.\n",
    "    lora_attn_modules=[\"q_proj\", \"v_proj\", \"output_proj\"],\n",
    "    # Whether to apply LoRA to the MLP in each transformer layer. Default: False\n",
    "    apply_lora_to_mlp=True,\n",
    "    # Whether to apply LoRA to the model's final output projection. Default: False\n",
    "    apply_lora_to_output=False,\n",
    "    # Rank of each low-rank approximation\n",
    "    rank=8,\n",
    "    # Scaling factor for the low-rank approximation\n",
    "    alpha=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:18:58.980625Z",
     "start_time": "2025-03-28T13:18:58.976472Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_config = TrainingConfigDataConfig(\n",
    "    # Identifier of the registered dataset for finetune\n",
    "    # Use client.datasets.list() to check all the available datasets\n",
    "    dataset_id=\"post_training_dataset\",\n",
    "    # Identifier of the registered dataset to validate the finetune model\n",
    "    # on validation_loss and perplexity\n",
    "    # Skip this if you don't want to run validatation on the model\n",
    "    validation_dataset_id=\"post_training_dataset\",\n",
    "    # Training data batch size\n",
    "    batch_size=8,\n",
    "    # Whether to shuffle the dataset.\n",
    "    shuffle=False,\n",
    "    # dataset format, select from ['instruct', 'dialog']\n",
    "    # change it to 'dialog' if you use dialog format dataset\n",
    "    data_format='instruct',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:15:49.622254Z",
     "start_time": "2025-03-27T14:15:49.619872Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_config = TrainingConfigOptimizerConfig(\n",
    "    # Currently only support adamw\n",
    "    optimizer_type=\"adamw\",\n",
    "    # Learning rate\n",
    "    lr=3e-4,\n",
    "    # adamw weight decay coefficient\n",
    "    weight_decay=0.1,\n",
    "    # The number of steps for the warmup phase for lr scheduler\n",
    "    num_warmup_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:16:55.415273Z",
     "start_time": "2025-03-27T14:16:55.413406Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "efficiency_config = TrainingConfigEfficiencyConfig(\n",
    "    # Help reduce memory by recalculating some intermediate activations\n",
    "    # during backward\n",
    "    enable_activation_checkpointing=True,\n",
    "    # We offer another memory efficiency flag called enable_activation_offloading\n",
    "    # which moves certain activations from GPU memory to CPU memory\n",
    "    # This further reduces GPU memory usage at the cost of additional\n",
    "    # data transfer overhead and possible slowdowns\n",
    "    # enable_activation_offloading=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:16:58.430184Z",
     "start_time": "2025-03-27T14:16:58.427843Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_config = TrainingConfig(\n",
    "    # num of training epochs\n",
    "    n_epochs=1,\n",
    "    data_config=data_config,\n",
    "    efficiency_config=efficiency_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    # max num of training steps per epoch\n",
    "    max_steps_per_epoch=10000,\n",
    "    # max num of steps for validation\n",
    "    max_validation_steps=10,\n",
    "    # Accumulate how many steps to calculate the gradient and update model parameters\n",
    "    # This is to simulate large batch size training while memory is limited\n",
    "    gradient_accumulation_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T16:41:24.075714Z",
     "start_time": "2025-03-28T16:41:24.060742Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "lls_client = LlamaStackClient(base_url=\"http://0.0.0.0:8321\", provider_data={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T16:56:40.715890Z",
     "start_time": "2025-03-28T16:56:40.687020Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# call supervised finetune API\n",
    "training_job = lls_client.post_training.supervised_fine_tune(\n",
    "    job_uuid=\"fine-tune-llm\",\n",
    "    # Base Llama model to be finetuned on\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    algorithm_config=algorithm_config,\n",
    "    # algorithm_config=None,\n",
    "    training_config=training_config,\n",
    "    # Base model checkpoint dir\n",
    "    # By default, the implementation will look at ~/.llama/checkpoints/<model>\n",
    "    checkpoint_dir=\"null\",\n",
    "    # logger_config and hyperparam_search_config haven't been supported yet\n",
    "    logger_config={},\n",
    "    hyperparam_search_config={},\n",
    ")\n",
    "\n",
    "pprint(training_job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
